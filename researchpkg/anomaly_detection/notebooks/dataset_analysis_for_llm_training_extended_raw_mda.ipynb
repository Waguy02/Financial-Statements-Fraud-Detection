{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary.\n",
    "--------------------------\n",
    "\n",
    "This notebook helps to analyse the dataset priori the llm training for fraud detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from itables import init_notebook_mode\n",
    "init_notebook_mode(all_interactive=True)\n",
    "import seaborn as sns\n",
    "# Set Seaborn style (optional)\n",
    "sns.set(style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    \"dataset_version\":\"v4\",\n",
    "    \"fold_id\":1 #FOLD_ID does not matter because the union of train and test will give us the entire dataset\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTENDED FIN DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from researchpkg.anomaly_detection.config import FINANCIALS_DIR_EXTENDED, PREPROCESSED_PATH_EXTENDED, SEED_TRAINING\n",
    "from researchpkg.anomaly_detection.models.utils import load_cross_validation_path\n",
    "from researchpkg.anomaly_detection.preprocessing.extended.sec_financial_preprocessing_quarterly_extended import EXTENDED_FINANCIAL_FEATURES, EXTENDED_FINANCIAL_FEATURES_COUNT_COLS\n",
    "import pandas as pd\n",
    "\n",
    "FULL_FINANCIAL_PATH = (\n",
    "    FINANCIALS_DIR_EXTENDED / \"sec_financials_quarterly.csv\"\n",
    ")\n",
    "\n",
    "def load_data( train_path=None, test_path=None):\n",
    "    \"\"\"\n",
    "    Load train and test datasets, merging with MDA and financial data.\n",
    "\n",
    "    Args:\n",
    "        train_path (Path, optional): Path to training data.\n",
    "        test_path (Path, optional): Path to test data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (train_df, val_df, test_df)\n",
    "    \"\"\"\n",
    "\n",
    "    def merge_with_financials(df, full_df):\n",
    "        \"\"\"Merges the financial data with the given DataFrame.\"\"\"\n",
    "        df = df.drop(columns=EXTENDED_FINANCIAL_FEATURES_COUNT_COLS, errors=\"ignore\")\n",
    "        df = df.merge(full_df, on=[\"cik\", \"year\", \"quarter\"], how=\"left\")\n",
    "        return df\n",
    "\n",
    "    train_path,test_path = load_cross_validation_path(CONFIG) \n",
    "\n",
    "    \n",
    "    full_df = pd.read_csv(FULL_FINANCIAL_PATH)\n",
    "    full_df = full_df[[\"cik\", \"year\", \"quarter\"] + EXTENDED_FINANCIAL_FEATURES]\n",
    "\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    train_df = merge_with_financials(train_df, full_df)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    test_df = merge_with_financials(test_df, full_df)\n",
    "\n",
    "    del full_df  # Clean up memory\n",
    "\n",
    "    return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "os.environ['TOKENIZERS_PARALLELISM']='1'\n",
    "model= \"unsloth/Llama-3.1-8B-Instruct-unsloth-bnb-4bit\"\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from researchpkg.anomaly_detection.config import PREPROCESSED_PATH,MDA_DATASET_PATH\n",
    "\n",
    "USE_RAW_MDA=True\n",
    "\n",
    "if USE_RAW_MDA:\n",
    "    MDA_PATH = MDA_DATASET_PATH /  \"quarterly\"\n",
    "else:\n",
    "    MDA_PATH = PREPROCESSED_PATH / \"SEC_MDA_SUMMARIZED\" / \"quarterly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from researchpkg.anomaly_detection.models.utils import drop_random_keys\n",
    "from researchpkg.anomaly_detection.preprocessing.extended.sec_financial_preprocessing_quarterly_extended import AGGREGATE_FEATURES, BENEISH_PROBM, DIFF_FEATURES, EXTENDED_FEATURES_SHORT_DESCRIPTION_DICT, EXTENDED_FINANCIAL_FEATURES, EXTENDED_FEATURES_DESCRIPTION_DICT, IMPORTANT_TAGS, RATIO_FEATURES\n",
    "from researchpkg.anomaly_detection.preprocessing.utils import clean_mda_content\n",
    "\n",
    "def load_mda_content(mda_quarter_id):\n",
    "        \"\"\"\n",
    "        Load the content of a MDA FIle.\n",
    "        \"\"\"\n",
    "\n",
    "        mda_file = MDA_PATH / f\"{mda_quarter_id}.txt\"\n",
    "\n",
    "        if not mda_file.exists():\n",
    "            raise FileNotFoundError(f\"MDA file {mda_file} does not exist.\")\n",
    "\n",
    "        with open(mda_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            return file.read()\n",
    "\n",
    "\n",
    "\n",
    "def truncate_prompt(mda_text: str, financials_str:str)-> str:\n",
    "        \n",
    "        # Truncate the part 2 of the prompt\n",
    "        prompt_tokens_part2 = TOKENIZER(\n",
    "            financials_str,\n",
    "            return_tensors=\"pt\",\n",
    "            padding =False,\n",
    "        )\n",
    "        \n",
    "        truncated_part2 = TOKENIZER.decode(\n",
    "            prompt_tokens_part2[\"input_ids\"][0],\n",
    "            skip_special_tokens=True,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # First tokenize and truncate the prompt (without the label)\n",
    "        prompt_tokens_part1 = TOKENIZER(\n",
    "            mda_text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=False\n",
    "            \n",
    "        )\n",
    "        \n",
    "        # Decode truncated prompt back to text\n",
    "        truncated_part1 = TOKENIZER.decode(\n",
    "            prompt_tokens_part1[\"input_ids\"][0],\n",
    "            skip_special_tokens=True)\n",
    "        \n",
    "        return truncated_part1, truncated_part2\n",
    "\n",
    "EXCLUDED_FINANCIALS_FEATURES = set([\n",
    "    BENEISH_PROBM # Too much biasing the model as it a probability of earnings manipulation\n",
    "])\n",
    "\n",
    "\n",
    "CURRENCY_FEATURES = set (AGGREGATE_FEATURES+DIFF_FEATURES+IMPORTANT_TAGS)\n",
    "def is_with_currency(feature):\n",
    "    return feature in CURRENCY_FEATURES\n",
    "\n",
    "PERCENTAGE_FEATURES = set(RATIO_FEATURES)\n",
    "\n",
    "\n",
    "def format_financials(financials, drop_rate=0):\n",
    "        \"\"\"\n",
    "        Format financial data dictionary into a string for the prompt.\n",
    "        Handles dropping features, formatting numbers, and adding units.\n",
    "\n",
    "        Args:\n",
    "            financials (dict): Dictionary of financial features {feature_name: value}.\n",
    "            drop_rate (float): Probability (0 to 1) of dropping each feature during formatting.\n",
    "\n",
    "        Returns:\n",
    "            str: Formatted string representation of the financials.\n",
    "        \"\"\"\n",
    "\n",
    "        def display_financial_value(value):\n",
    "            \"\"\"Formats financial values for display.\"\"\"\n",
    "            if pd.isna(value):\n",
    "                return \"N/A\" # Handle missing values explicitly\n",
    "            try:\n",
    "                value = float(value)\n",
    "                if value == 0:\n",
    "                    return \"0\"\n",
    "                elif abs(value) < 0.01 and abs(value) > 0: # Small non-zero values\n",
    "                    return f\"{value:.2e}\"\n",
    "                elif abs(value) < 10:\n",
    "                    return f\"{value:.2f}\"\n",
    "                else:\n",
    "                    # Format with commas, no decimal places for large numbers\n",
    "                    return \"{:,.0f}\".format(value)\n",
    "            except (ValueError, TypeError):\n",
    "                return str(value) # Return as string if not convertible to float\n",
    "\n",
    "        # Filter out excluded features and invalid values (NaN, Inf)\n",
    "        processed_financials = {}\n",
    "        for k, v in financials.items():\n",
    "            if k not in EXCLUDED_FINANCIALS_FEATURES and pd.notna(v) and np.isfinite(v) and v != 0:\n",
    "                 processed_financials[k] = v\n",
    "\n",
    "\n",
    "        # Apply feature dropout if requested\n",
    "        if drop_rate > 0:\n",
    "            processed_financials = drop_random_keys(processed_financials, drop_rate)\n",
    "\n",
    "        # Format the remaining features into strings\n",
    "        financial_lines = []\n",
    "        # Sort for consistency (optional)\n",
    "        sorted_keys = sorted(processed_financials.keys())\n",
    "\n",
    "        for key in sorted_keys:\n",
    "            value = processed_financials[key]\n",
    "            description = EXTENDED_FEATURES_SHORT_DESCRIPTION_DICT.get(key, key) # Use key if description missing\n",
    "            unit = \"\"\n",
    "            formatted_value = value # Start with original value\n",
    "\n",
    "            # Apply specific formatting based on feature type\n",
    "            if key in PERCENTAGE_FEATURES:\n",
    "                unit = \"%\"\n",
    "                formatted_value = value * 100 # Convert ratio to percentage\n",
    "            elif is_with_currency(key):\n",
    "                unit = \"$\" # Assume USD, adjust if needed\n",
    "\n",
    "            # Format the number using the helper function\n",
    "            display_value = display_financial_value(formatted_value)\n",
    "\n",
    "            # Add unit prefix/suffix\n",
    "            if unit == \"$\":\n",
    "                display_str = f\"{unit}{display_value}\"\n",
    "            elif unit == \"%\":\n",
    "                 display_str = f\"{display_value}{unit}\"\n",
    "            else:\n",
    "                 display_str = display_value # No unit\n",
    "\n",
    "\n",
    "            financial_lines.append(f\"- {description}: {display_str}\")\n",
    "\n",
    "        return \"\\n\".join(financial_lines) if financial_lines else \"No financial data available.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_sample_data(row):\n",
    "    \"\"\"\n",
    "    Returns a sample of the training data for testing purposes.\n",
    "    And return : \n",
    "    - {\n",
    "        \n",
    "        \"prompt_part1:'\"The content of the first part of the prompt, which includes the MDA text.\"\n",
    "        \"prompt_part2: \"The content of the second part of the prompt, which includes the financials.\"\n",
    "        \"full_text\":\n",
    "        \n",
    "        \"prompt_part1_tokens_count\":\n",
    "        \"prompt_part2_tokens_count\":\n",
    "        \"full_text_tokens_count\":\n",
    "        \n",
    "        \n",
    "        \"prompt_part1_truncated\":\n",
    "        \"prompt_part2_truncated\":\n",
    "        \"full_text_truncated\":\n",
    "        \n",
    "        \"prompt_part1_truncated_tokens_count\":\n",
    "        \"prompt_part2_truncated_tokens_count\":\n",
    "        \"full_text_truncated_tokens_count\":\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the MDA quarter ID\n",
    "    mda_quarter_id = row[\"mda_quarter_id\"]\n",
    "    mda_content = load_mda_content(mda_quarter_id)\n",
    "    label = \"Fraud\" if row[\"is_fraud\"] else \"Not Fraud\"\n",
    "\n",
    "    \n",
    "    # Get the financials for the current row\n",
    "    # Convert the financials dictionary to a string format\n",
    "    financials = row[EXTENDED_FINANCIAL_FEATURES].to_dict()\n",
    "    financials_str = format_financials(financials)\n",
    "    \n",
    "    \n",
    "    #Compute the tokens count\n",
    "    \n",
    "    full_text = mda_content + financials_str\n",
    "    full_text_tokens_count = len(TOKENIZER(full_text)[\"input_ids\"])\n",
    "    \n",
    "    prompt_part1 = mda_content\n",
    "    prompt_part1_tokens_count = len(TOKENIZER(prompt_part1)[\"input_ids\"])\n",
    "    \n",
    "    prompt_part2 = financials_str\n",
    "    prompt_part2_tokens_count = len(TOKENIZER(prompt_part2)[\"input_ids\"])\n",
    "    \n",
    "    \n",
    "    truncated_part1, truncated_part2 = truncate_prompt(mda_content, financials_str)\n",
    "    prompt_part1_truncated_tokens_count = len(TOKENIZER(truncated_part1)[\"input_ids\"])\n",
    "    prompt_part2_truncated_tokens_count = len(TOKENIZER(truncated_part2)[\"input_ids\"])\n",
    "    \n",
    "    # Truncate the full text\n",
    "    full_text_truncated = truncated_part1 + truncated_part2 + \"<start_of_turn>model\\n\" + label\n",
    "    \n",
    "    full_text_truncated_tokens_count = len(TOKENIZER(full_text_truncated)[\"input_ids\"])\n",
    "    # Return the sample data\n",
    "    \n",
    "    return {\n",
    "        \"prompt_part1\": prompt_part1,\n",
    "        \"prompt_part2\": prompt_part2,\n",
    "        \"full_text\": full_text,\n",
    "        \n",
    "        \"prompt_part1_tokens_count\": prompt_part1_tokens_count,\n",
    "        \"prompt_part2_tokens_count\": prompt_part2_tokens_count,\n",
    "        \"full_text_tokens_count\": full_text_tokens_count,\n",
    "        \n",
    "        \n",
    "        \"prompt_part1_truncated\": truncated_part1,\n",
    "        \"prompt_part2_truncated\": truncated_part2,\n",
    "        \"full_text_truncated\": full_text_truncated,\n",
    "        \n",
    "        \"prompt_part1_truncated_tokens_count\": prompt_part1_truncated_tokens_count,\n",
    "        \"prompt_part2_truncated_tokens_count\": prompt_part2_truncated_tokens_count,\n",
    "        \"full_text_truncated_tokens_count\": full_text_truncated_tokens_count\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe with the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import numpy as np  # Import numpy\n",
    "\n",
    "\n",
    "def process_row(row, subset):\n",
    "    \"\"\"Processes a single row and returns a dictionary of token statistics.\"\"\"\n",
    "    # Get the sample data\n",
    "    sample_data = get_sample_data(row)\n",
    "\n",
    "    # Create a dictionary with the results\n",
    "    results = {\n",
    "        \"cik\": row[\"cik\"],\n",
    "        \"year\": row[\"year\"],\n",
    "        \"quarter\": row[\"quarter\"],\n",
    "        \"is_fraud\": row[\"is_fraud\"],\n",
    "        \"mda_quarter_id\": row[\"mda_quarter_id\"],\n",
    "        \"subset\": subset,\n",
    "        **sample_data\n",
    "    }\n",
    "    return results\n",
    "\n",
    "\n",
    "def process_partition(partition, subset):\n",
    "    \"\"\"Processes a chunk of the DataFrame.\"\"\"\n",
    "    import tqdm.notebook as tqdm\n",
    "    results = []\n",
    "    for index, row in tqdm.tqdm(partition.iterrows(), desc=f\"Processing {subset} partition\", total=len(partition)):\n",
    "        results.append(process_row(row, subset))\n",
    "    return results\n",
    "\n",
    "\n",
    "def parallel_process_dataframe(df, subset, num_processes=None):\n",
    "    \"\"\"\n",
    "    Parallelizes the processing of a single DataFrame by partitioning it.\n",
    "\n",
    "    Args:\n",
    "        df: The DataFrame to process.\n",
    "        subset: The subset name (\"train\" or \"test\").\n",
    "        num_processes: Number of processes to use. If None, uses the number of CPU cores.\n",
    "\n",
    "    Returns:\n",
    "        A DataFrame containing the combined results.\n",
    "    \"\"\"\n",
    "    if num_processes is None:\n",
    "        num_processes = cpu_count()\n",
    "\n",
    "    # Split the DataFrame into partitions\n",
    "    partitions = np.array_split(df, num_processes)\n",
    "\n",
    "    with Pool(num_processes) as pool:\n",
    "        results = pool.starmap(process_partition, [(partition, subset) for partition in partitions])\n",
    "\n",
    "    # Combine the results\n",
    "    all_results = []\n",
    "    for result_list in results:\n",
    "        all_results.extend(result_list)\n",
    "\n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "\n",
    "def parallel_process_data(train_df, test_df, num_processes=None):\n",
    "    \"\"\"\n",
    "    Processes train_df and test_df separately, each in parallel.\n",
    "\n",
    "    Args:\n",
    "        train_df: DataFrame containing training data.\n",
    "        test_df: DataFrame containing testing data.\n",
    "        num_processes: Number of processes to use. If None, uses the number of CPU cores.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame containing the combined token statistics for train and test data.\n",
    "    \"\"\"\n",
    "    if num_processes is None:\n",
    "        num_processes = cpu_count()\n",
    "\n",
    "    train_results_df = parallel_process_dataframe(train_df, \"train\", num_processes)\n",
    "    test_results_df = parallel_process_dataframe(test_df, \"test\", num_processes)\n",
    "\n",
    "    df_tokens_stats = pd.concat([train_results_df, test_results_df], ignore_index=True)\n",
    "    return df_tokens_stats\n",
    "\n",
    "df_tokens_stats = parallel_process_data(train_df, test_df, num_processes=cpu_count()-2)\n",
    "df_tokens_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokens_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokens_stats[\"prompt_part1_tokens_count\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20, 6))\n",
    "sns.histplot(data=df_tokens_stats, x=\"prompt_part1_tokens_count\", bins=50, kde=True, color=\"purple\")\n",
    "plt.xticks(fontsize=18)\n",
    "plt.xlabel(\"Number of Tokens in raw quarterly MD&A sections\",fontsize=18)\n",
    "plt.ylabel(\"Frequency\",fontsize=18) \n",
    "plt.yticks(fontsize=18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6))\n",
    "sns.histplot(data=df_tokens_stats, x=\"prompt_part2_tokens_count\", bins=50, kde=True, color=\"darkblue\")\n",
    "plt.xticks(fontsize=18)\n",
    "plt.xlabel(\"Number of Tokens of financial features_prompts\",fontsize=18)\n",
    "plt.ylabel(\"Frequency\",fontsize=18) \n",
    "plt.yticks(fontsize=18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokens_stats[\"prompt_part2_tokens_count\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_RAW_MDA:\n",
    "    df_tokens_stats.to_csv(PREPROCESSED_PATH / \"EXTENDED/mda_raw_token_stats.csv\", index=False)\n",
    "else:\n",
    "    df_tokens_stats.to_csv(PREPROCESSED_PATH / \"EXTENDED/mda_summarized_token_stats.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the tokens stats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokens_stats[\"full_text_tokens_count\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokens_stats[\"full_text_truncated_tokens_count\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out of scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_test = df_tokens_stats.sort_values([\"prompt_part1_tokens_count\"], ascending=False).reset_index(drop=True)[\"full_text\"][0]\n",
    "longest_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Industry sectors distribution the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "index_file= PREPROCESSED_PATH_EXTENDED/\"v4/global_stats.yaml\"\n",
    "dataset_config = yaml.load(open(index_file),Loader=yaml.Loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_sector_distribution  = dataset_config[\"sic_distribution\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_sector_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_raw = per_sector_distribution['global']\n",
    "fraud_raw = per_sector_distribution['fraud']\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "wrapper = textwrap.TextWrapper(width=14)\n",
    "do_wrap = lambda x : \"\\n\".join(wrapper.wrap(x))\n",
    "global_= dict(sorted(global_raw.items(),key=lambda x:x[1],reverse=True))\n",
    "global_ = {do_wrap(k):v for k,v in global_.items()}\n",
    "\n",
    "\n",
    "fraud_ = {do_wrap(k):v for k,v in fraud_raw.items()}\n",
    "fraud_= {(k):fraud_[k] for k in global_ if k in fraud_}\n",
    "\n",
    "# Filter only sectors present in fraud\n",
    "sectors = list(fraud_.keys())\n",
    "\n",
    "print(\"sectors\", fraud_)\n",
    "\n",
    "\n",
    "\n",
    "fraud_counts = [fraud_[sector] for sector in sectors]\n",
    "global_counts = [global_[sector] for sector in sectors]\n",
    "fraud_percentages = [f\"{fraud_[sector]/global_[sector]*100:.1f}%\" for sector in sectors]\n",
    "\n",
    "\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(13, 5))\n",
    "palette = sns.color_palette(\"hls\", len(sectors))\n",
    "bars = sns.barplot(x=sectors, y=global_counts, palette=palette)\n",
    "\n",
    "# Add annotations\n",
    "for bar, count, fraud_num, pct in zip(bars.patches, global_counts, fraud_counts, fraud_percentages):\n",
    "    bar_x = bar.get_x() + bar.get_width() / 2\n",
    "    bar_y = bar.get_height()\n",
    "    y_pos  =bar_y/1.5 \n",
    "    \n",
    "    if fraud_num==1:\n",
    "        y_pos = bar_y*4.5\n",
    "    elif fraud_num==16:\n",
    "        y_pos = bar_y*2    \n",
    "    \n",
    "    \n",
    "    label = f\"{fraud_num} F.\\n({pct})\"\n",
    "    plt.text(bar_x, y_pos, label, ha='center', va='center', fontsize=20, color='black', fontfamily=\"Nimbus Sans\" ,fontweight=600)\n",
    "\n",
    "plt.ylabel(\"\")\n",
    "plt.xticks(rotation=0, ha='center',fontsize=18, fontfamily=\"Nimbus Sans\", fontweight=600)\n",
    "plt.yticks(fontsize=20, fontfamily=\"Nimbus Sans\", fontweight=600)\n",
    "# plt.title(\"Fraud cases per sector (count and percentage over total)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
